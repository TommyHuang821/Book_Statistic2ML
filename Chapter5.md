---
title: 'Book_Statistic2ML_Chapter5'
disqus: hackmd
---
[讀者天地 - 機器學習的統計基礎](https://hackmd.io/@TommyHuang/book_statistics2ML)
> [name=黃志勝 Chih-Sheng (Tommy) Huang]
* [作者Medium Blog](https://chih-sheng-huang821.medium.com/)

本書「機器學習的統計基礎」出版社為旗標科技股份有限公司

**此份筆記為[讀者天地 - 機器學習的統計基礎](https://hackmd.io/@TommyHuang/book_statistics2ML)用來作為書本內容修正紀錄**

---
[TOC]
# 第 5 章 機器與深度學習常用到的基礎理論
## 5.1 機器、深度學習與統計學關係
### 5.1.1 統計學與機器學習(深度學習)的差異
### 5.1.2 機器學習和深度學習的差異
## 5.2 監督式學習與非監督式學習
### 5.2.1 監督式學習(Supervised Learning)
### 5.2.2 非監督式學習(Unsupervised Learning)
## 5.3 最大概似估計
### 5.3.1 概似函數的定義
### 5.3.2 範例一：伯努利抽紅白球的機率
### 5.3.3 範例二：常態分布找出平均值與變異數
## 5.4 貝葉斯法則理論與最大後驗機率
### 5.4.1 貝葉斯法則理論
### 5.4.2 最大後驗機率法
## 5.5 常用到的距離和相似度計算方式
### 5.5.1 曼哈頓距離（Manhattan Distance）
### 5.5.2 歐幾里得距離(Euclidean Distance)
### 5.5.3 明可夫斯基距離（Minkowski distance）
### 5.5.4 餘弦相似度(Cosine similarity)
### 5.5.5 馬氏距離(Mahalanobis Distance)
### 5.5.6 雅卡爾相似度係數(Jaccard similarity coefficient)
## 5.6 損失函數
### 5.6.1 回歸常用損失函數：均方誤差、平均絕對值誤差
### 5.6.2 回歸常用損失函數：Huber損失函數
### 5.6.3 分類常用損失函數：交叉熵
### 5.6.4 交叉熵與相對熵、最大概似估計的關係



###### tags: `Book_Statistic2ML`
